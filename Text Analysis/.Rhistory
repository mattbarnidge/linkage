# 1) Variables: a) see fox story, b) fb users + open-ended fox + see fox story
# 2) Data filters: a) FB users and or SM users?
# 3) Outcomes: Two factors for Trust, 3 factors for polarization, negative expression
#WD and Data
setwd("~/Desktop/Fox Effect")
#coded/cleaned data from time of collection (variable creator +  valence of open-ended news items)
load("~/Desktop/Fox Effect/EMCP20_coded.Rdata")
library(ltm)
library(dplyr)
library(lme4)
library(lmerTest)
#Outcomes Variables
#Polarization (difference score)
table(d$ap.cand)
hist(d$ap.cand)
table(d$ap.party)
hist(d$ap.party)
table(d$ap.ideo)
hist(d$ap.ideo)
with(d, cronbach.alpha(cbind(ap.cand, ap.ideo, ap.party), na.rm = TRUE)) # alpha = .845
d$ap = with(d, rowMeans(cbind(ap.cand, ap.ideo, ap.party), na.rm = TRUE))
hist(d$ap)
#Trust: social 2-item social trust and 2-item media trust
table(d$soctrust)
hist(d$soctrust)
table(d$medtrust)
hist(d$medtrust)
#Negative expression (six-item negative expression scale)
table(d$neg)
hist(d$neg)
#network vars
d$size = log(d$sm.ns) #network size, logged
d$div = d$sm.div.tie #Eight-item social media tie diversity scale
#Fox News
#Variables
#story.part (fox/non fox story 1 = Fox)
d$recall = d$story.aware #recall of embedded story (state)
#clean_news1 (cleaned open-ended questions 1, 2, 3)
#smnews.fb	(Facebook news use variable)
#sm.freq (overall social media use)
#ID  open-ended FOX responses
d$fox1 = ifelse(d$clean_news1 == 'fox', 1, 0)
d$fox2 = ifelse(d$clean_news2 == 'fox', 1, 0)
d$fox3 = ifelse(d$clean_news3 == 'fox', 1, 0)
d$fox4 = (d$fox1 + d$fox2 + d$fox3)
table(d$fox4)
d$fox_open = ifelse(d$fox4 > 0, 1,0) # 1 = fox
table(d$fox_open)
#Groups: FB news users + open-ended fox response + story aware or got the fox story
d$sm_foxrecall = ifelse(d$smnews.fb > 1 & d$fox_open > 0 & d$story.part, 1, 0) #n = 255
d$sm_recall = ifelse(d$smnews.fb > 1 & d$fox_open > 0 & d$story.aware, 1, 0) # n = 244
d$sm_fox = ifelse(d$smnews.fb > 1 & d$fox_open > 0, 1, 0) # n = 507
table(d$sm_foxrecall)
table(d$sm_recall)
table(d$sm_fox)
#Libraries
library(readxl)
library(quanteda) #might also need to update Rcpp package if run into problems
library(quanteda.textstats)
load("/Users/trevordiehl/Documents/Research/Survey F20/Text cleaning and analysis/open_responses/news_master.Rdata")
View(txt)
load("/Users/trevordiehl/Documents/Research/Survey F20/EMCP20/EMCP20_/Data/news_posts.Rdata")
d = sample(posts, 500, replace = F, prob = NULL)
d = sample(posts, 500, replace = T, prob = NULL)
d = sample[posts(sample(nrow)df), size = 500), ]
d = sample[posts(sample(nrow(posts)), size = 500), ]
d = posts[posts(sample(nrow(posts)), size = 500), ]
d = posts[sample(nrow(posts)), size = 500), ]
d = posts[sample(nrow(posts), size = 500), ]
View(d)
d = posts(2:13, )
d = posts[2:13, ]
d = posts[sample(nrow(posts), size = 500), ]
d = posts[,2:13]
View(d)
setwd("~/Desktop/Text Analysis")
save(posts, file = "sample_posts.Rdata")
library(readxl) #reading in XL docs
library(quanteda) #might also need to update Rcpp package if run into problems
library(quanteda.textstats) #newer package for stats
library(tm)
#new variable for text analysis
d1$text = d1$snippet
setwd("~/Desktop/Text Analysis")
load("~/Desktop/Text Analysis/sample_posts.Rdata")
d = posts[sample(nrow(posts), size = 500), ]
d = posts[,2:13]
d = posts[,2:13]
d = posts[sample(nrow(posts), size = 500), ]
d = posts[,2:13]
d = posts[sample(nrow(posts), size = 500), ]
d = d[,2:13]
save(d, file = "sample_posts.Rdata")
#coded/cleaned data from time of collection (variable creator +  valence of open-ended news items)
load("~/Desktop/AP INE/EMCP20_coded.Rdata")
#data & WD
#setwd("~/Desktop/Text Analysis")
load("~/Desktop/Text Analysis/sample_posts.Rdata")
d1 = d
#new variable for text analysis
d1$text = d1$snippet
#quick manual standardization of text
d1$text = removePunctuation(d1$text) #remove punctuation
d1$text = char_tolower(d1$text) #lower case
d1$text = gsub(' j |-|[^\x01-\x7F]', " ", d1$text)#remove dashes and emoticons (gsub uses grep commands)
#sentiment Analysis
#create document feature matrix dfm) and apply the counts
sent_dfm = dfm(d1$text, dictionary = data_dictionary_LSD2015) #LSD = Lexicoder Sentiment Dictionary (built in package!)
head(sent_dfm) #check the 'head' of the matrix
#convert document feature matrix back into the data frame, merge sentiment counts to the data
d2 = convert(sent_dfm, to = "data.frame")
d1 = cbind(d1, d2[ ,2:5])
#a handful of stop words are in the LSD, so truncate after to get more accurate word count
d1$text = removeWords(d1$text, stopwords(kind = "en"))
d1$wc = ntoken(d1$text)
#net sentiment based on previous work
d1$net_sent = ((d1$positive - d1$neg_positive)/d1$wc) - ((d1$negative - d1$neg_negative)/d1$wc)
rm(d2, sent_dfm)
View(d1)
#data & WD
#setwd("~/Desktop/Text Analysis")
load("~/Desktop/Text Analysis/sample_posts.Rdata")
#new variable for text analysis
d$text = d$snippet
#quick manual standardization of text
d$text = removePunctuation(d$text) #remove punctuation
d$text = char_tolower(d$text) #lower case
d$text = gsub(' j |-|[^\x01-\x7F]', " ", d$text)#remove dashes and emoticons (gsub uses grep commands)
#inspect
d$text[,1:5]
View(d)
#inspect
d$text[ ,3,12]
#inspect
d$text[ ,c(3,12)]
#inspect
d[ ,c(3,12)]
#inspect
d[ 1:4,c(3,12)]
#inspect
d[ 1,c(3,12)]
View(d)
#inspect
d[ 1:2,c(3,12)]
#data & WD
#setwd("~/Desktop/Text Analysis")
load("~/Desktop/Text Analysis/sample_posts.Rdata")
#new variable for text analysis
d$text = d$snippet
#quick manual standardization of text
d$text = removePunctuation(d$text) #remove punctuation
d$text = char_tolower(d$text) #lower case
d$text = gsub(' j |-|[^\x01-\x7F]', " ", d$text)#remove dashes and emoticons (gsub uses grep commands)
#quick inspect cleaning
d[ 1,c(3,12)]
#sentiment Analysis
#create document feature matrix dfm) and apply the counts
sent_dfm = dfm(d$text, dictionary = data_dictionary_LSD2015) #LSD = Lexicoder Sentiment Dictionary (built in package!)
#sentiment Analysis
#create document feature matrix dfm) and apply the counts
sent_dfm = tokens(d$text, dictionary = data_dictionary_LSD2015) #LSD = Lexicoder Sentiment Dictionary (built in package!)
#sentiment Analysis
#create document feature matrix dfm) and apply the counts
sent_dfm = tokens(d$text, dfm_lookup = data_dictionary_LSD2015) #LSD = Lexicoder Sentiment Dictionary (built in package!)
#sentiment Analysis
#create document feature matrix dfm) and apply the counts
sent_dfm = tokens(d$text, dfm_lookup(data_dictionary_LSD2015) )#LSD = Lexicoder Sentiment Dictionary (built in package!)
#sentiment Analysis
#create document feature matrix dfm) and apply the counts
sent_dfm = dfm(d$text, dictionary = data_dictionary_LSD2015) #LSD = Lexicoder Sentiment Dictionary (built in package!)
head(sent_dfm) #check the 'head' of the matrix
#convert document feature matrix back into the data frame, merge sentiment counts to the data
d2 = convert(sent_dfm, to = "data.frame")
d = cbind(d, d2[ ,2:5])
#a handful of stop words are in the LSD, so truncate after to get more accurate word count
d$text = removeWords(d$text, stopwords(kind = "en"))
d$wc = ntoken(d$text)
#net sentiment based on previous work
d$net_sent = ((d$positive - d$neg_positive)/d$wc) - ((d$negative - d$neg_negative)/d$wc)
rm(d2, sent_dfm)
View(d)
#data & WD
#setwd("~/Desktop/Text Analysis")
load("~/Desktop/Text Analysis/sample_posts.Rdata")
#new variable for text analysis
d$text = d$snippet
#quick manual standardization of text
d$text = removePunctuation(d$text) #remove punctuation
d$text = char_tolower(d$text) #lower case
d$text = gsub(' j |-|[^\x01-\x7F]', " ", d$text)#remove dashes and emoticons (gsub uses grep commands)
#quick inspect cleaning
d[ 1,c(3,12)]
#sentiment Analysis
#create document feature matrix dfm) and apply the counts
sent_dfm = dfm(d$text, dictionary = data_dictionary_LSD2015) #LSD = Lexicoder Sentiment Dictionary (built in package!)
head(sent_dfm) #check the 'head' of the matrix
??stopwords
#data & WD
#setwd("~/Desktop/Text Analysis")
load("~/Desktop/Text Analysis/sample_posts.Rdata")
#new variable for text analysis
d$text = d$snippet
#quick manual standardization of text
d$text = removePunctuation(d$text) #remove punctuation
d$text = char_tolower(d$text) #lower case
d$text = gsub(' j |-|[^\x01-\x7F]', " ", d$text)#remove dashes and emoticons (gsub uses grep commands)
#quick inspect cleaning
d[ 1,c(3,12)]
#create document feature matrix dfm) and apply the counts
#getting warnings b/c the package updated and commands are being phased out
sent_dfm = dfm(d$text, dictionary = data_dictionary_LSD2015) #LSD = Lexicoder Sentiment Dictionary (built in package!)
head(sent_dfm) #check the 'head' of the matrix
#convert document feature matrix back into the data frame, merge sentiment counts to the data
d2 = convert(sent_dfm, to = "data.frame")
d = cbind(d, d2[ ,2:5])
#a handful of stop words are in the LSD, so truncate before setting word count
d$text = removeWords(d$text, stopwords(kind = "en"))#remove stop words
d$wc = ntoken(d$text)#word count
#net sentiment based on previous work
d$net_sent = ((d$positive - d$neg_positive)/d$wc) - ((d$negative - d$neg_negative)/d$wc)
rm(d2, sent_dfm)
View(d)
library(readxl)
t1 = read_excel("~/Desktop/Papers/News Engagement/Trial Coding/Cleaned Coded/trial_3_master.xls")
t1 = read_excel("~/Documents/Papers/News Engagement/Trial Coding/Cleaned Coded/trial_3_master.xls")
setwd("~/Documents/Research/Papers/News Engagement")
t1 = read_excel("~/Documents/Research/Papers/News Engagement/Trial Coding/Cleaned Coded/trial_3_master.xls")
View(t1)
t2 = read_excel("~/Documents/Research/Papers/News Engagement/Trial Coding/Cleaned Coded/trial_4_Master.xlsx")
t3 = read_excel("~/Documents/Research/Papers/News Engagement/Trial Coding/Cleaned Coded/trial_2_Master.xls")
d1 = rbind(t1,t2,t3)
rm(t1, t2,t3)
View(d1)
View(d1)
#Data & WD
setwd("~/Desktop/Text Analysis")
d_coded = d1
save(d_coded, file = "coded_posts.Rdata")
load("~/Desktop/Text Analysis/coded_posts.Rdata")
d1 = d_coded
#Create new variable that collapses the title, description, and text into one cell#
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells#
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
#Clean text
d1$text2 = removePunctuation(d$text2) #remove punctuation
#Clean text
d1$text2 = removePunctuation(d1$text2) #remove punctuation
d$text2 = char_tolower(d1$text2) #lower case
d1$text2 = char_tolower(d1$text2) #lower case
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove dashes and emoticons (gsub uses grep commands)
d1$text = removeWords(d1$text2, stopwords(kind = "en"))#remove stop words
#Check text
d1$headline[11]
View(d1)
#Check text
d1$headline[11]
d1$text2
#Check text
d1$text2[2]
View(d1)
#Clean text
d1$text2 = removePunctuation(d1$text2) #remove punctuation
d1$text2 = char_tolower(d1$text2) #lower case
d1$text2 = gsub("-", " ", d1$text2)#replace dashes w/spaces
d1$text2 = gsub("null", "", d1$text2) #remove
d1$text2 = gsub("na", "", d1$text2) #remove
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove punct. and emoticons
d1$text = removeWords(d1$text2, stopwords(kind = "en"))#remove stop words
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
#Clean text
d1$text2 = removePunctuation(d1$text2) #remove punctuation
d1$text2 = char_tolower(d1$text2) #lower case
d1$text2 = gsub("-", " ", d1$text2)#replace dashes w/spaces
d1$text2 = gsub("null", "", d1$text2) #remove
d1$text2 = gsub("na", "", d1$text2) #remove
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove punct. and emoticons
d1$text = removeWords(d1$text2, stopwords(kind = "en"))#remove stop words
#Check text
d1$text2[2]
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check text
d1$text2[2]
d1$text2 = gsub("-", " ", d1$text2)#replace dashes w/spaces
#Check text
d1$text2[2]
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove punct. and emoticons
#Check text
d1$text2[2]
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check text
d1$text2[2]
#Clean text
d1$text2 = removePunctuation(d1$text2) #remove punctuation
#Check text
d1$text2[2]
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
#Clean text (order of cleaning matters, so see that the process is working)
d1$text2 = char_tolower(d1$text2) #lower case
d1$text2 = gsub("-", " ", d1$text2)#replace dashes w/spaces
d1$text2 = gsub("null", "", d1$text2) #remove
d1$text2 = gsub("na", "", d1$text2) #remove
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove emoticons
d1$text2 = removePunctuation(d1$text2) #remove other punctuation
d1$text2 = removeWords(d1$text2, stopwords(kind = "en"))#remove stop words
#Check text
d1$text2[2]
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
#Clean text (order of cleaning matters, so see that the process is working)
d1$text2 = char_tolower(d1$text2) #lower case
d1$text2 = gsub("-", " ", d1$text2)#replace dashes w/spaces
d1$text2 = gsub("null", "", d1$text2) #remove
d1$text2 = gsub("na", "", d1$text2) #remove
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove emoticons
d1$text2 = removePunctuation(d1$text2) #remove other punctuation
d1$text2 = removeWords(d1$text2, stopwords(kind = "en"))#remove stop words
#Check text
d1$text2[2]
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check text
d1$text2[2]
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Clean text (order of cleaning matters, so see that the process is working)
d1$text2 = char_tolower(d1$text2) #lower case
d1$text2 = gsub("-", " ", d1$text2)#replace dashes w/spaces
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove emoticons
d1$text2 = removePunctuation(d1$text2) #remove other punctuation
d1$text2 = removeWords(d1$text2, stopwords(kind = "en"))#remove stop words
#Check text
d1$text2[2]
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check text
d1$text2[2]
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
#Check text
d1$text2[2]
View(d1)
load("~/Desktop/Text Analysis/coded_posts.Rdata") #750 human coded posts, 5 categories of News Drivers
d1 = d_coded
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
#Clean text (order of cleaning matters, so see that the process is working)
d1$text2 = char_tolower(d1$text2) #lower case
d1$text2 = gsub("-", " ", d1$text2)#replace dashes w/spaces
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove emoticons
d1$text2 = removePunctuation(d1$text2) #remove other punctuation
d1$text2 = removeWords(d1$text2, stopwords(kind = "en"))#remove stop words
#Check text
d1$text2[2]
load("~/Desktop/Text Analysis/coded_posts.Rdata") #750 human coded posts, 5 categories of News Drivers
View(d1)
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
d1$text2 <- tolower(d1$text2)
d1$text2 = removeWords(d1$text2, stopwords("english"))
d1$text2 = gsub("-", " ", d1$text2)
d1$text2 = gsub("null", "", d1$text2)
d1$text2 = gsub("na", "", d1$text2)
#Check text
d1$text2[2]
load("~/Desktop/Text Analysis/coded_posts.Rdata") #750 human coded posts, 5 categories of News Drivers
d1 = d_coded
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
#Clean text (order of cleaning matters, so see that the process is working)
d1$text2 = char_tolower(d1$text2) #lower case
d1$text2 = gsub("-", " ", d1$text2)#replace dashes w/spaces
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove emoticons
d1$text2 = removePunctuation(d1$text2) #remove other punctuation
d1$text2 = removeWords(d1$text2, stopwords(kind = "en"))#remove stop words
#Check text
d1$text2[2]
d1$text2 = removeNumbers(d1$text2)
#Check text
d1$text2[2]
#2nd step: create the folded variable and run test (f = folded variable)
d1$f = 0
d1$f[d1$event==1] <- "1"
d1$f[d1$entertainment==1] <- "2"
d1$f[d1$lifestyle==1] <- "3"
d1$f[d1$report==1] <- "4"
d1$f[d1$government==1] <- "5"
d1$f[d1$`dont know`== 1] <- "6"
d1$f = as.numeric(d1$f)
View(d1)
#create single variable (f = folded variable)
d1$f = 0
load("~/Desktop/Text Analysis/coded_posts.Rdata") #750 human coded posts, 5 categories of News Drivers
d1 = d_coded
#1 Pre-processing
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
#Clean text (order of cleaning matters, so check that the cleaning process is working)
d1$text2 = gsub("\NA", " ", d1$text2)
#Clean text (order of cleaning matters, so check that the cleaning process is working)
d1$text2 = gsub("NA", " ", d1$text2)
#Check text
d1$text2[2]
View(d1)
load("~/Desktop/Text Analysis/coded_posts.Rdata") #750 human coded posts, 5 categories of News Drivers
d1 = d_coded
#1 Pre-processing
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
#Clean text (order of cleaning matters, so check that the cleaning process is working)
d1$text2 = gsub("NA", " ", d1$text2)
d1$text2 = gsub("NULL", " ", d1$text2)
d1$text2 = char_tolower(d1$text2) #lower case
d1$text2 = gsub("-", " ", d1$text2)#replace dashes w/spaces
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove emoticons
d1$text2 = removePunctuation(d1$text2) #remove other punctuation
d1$text2 = removeWords(d1$text2, stopwords(kind = "en"))#remove stop words
d1$text2 = removeNumbers(d1$text2) #numbers
#Check text
d1$text2[2]
View(d1)
load("~/Desktop/Text Analysis/coded_posts.Rdata") #750 human coded posts, 5 categories of News Drivers
d1 = d_coded
#1 Pre-processing
#Create new variable that collapses the title, description, and text into one cell
d1$text2 <- paste(d1$headline, d1$text, d1$description, sep=" ")
#Check our new cell versus prior cells
d1$headline[1] ; d1$text[1] ; d1$description[1]  ; d1$text2[1]
#Clean text (order of cleaning matters, so check that the cleaning process is working)
d1$text2 = gsub("NA", "", d1$text2) #replace NAs
d1$text2 = gsub("NULL", "", d1$text2)
d1$text2 = char_tolower(d1$text2) #lower case
d1$text2 = gsub("-", " ", d1$text2)#replace dashes w/spaces
d1$text2 = gsub(' j |-|[^\x01-\x7F]', " ", d1$text2)#remove emoticons
d1$text2 = removePunctuation(d1$text2) #remove other punctuation
d1$text2 = removeWords(d1$text2, stopwords(kind = "en"))#remove stop words
d1$text2 = removeNumbers(d1$text2) #numbers
#Check text
d1$text2[2]
View(d1)
#create single variable (f = folded variable); 1 = Hard News; 2 = Soft News
d1$f = 0
d1$f[d1$event==1] <- "1"
d1$f[d1$entertainment==1] <- "2"
d1$f[d1$lifestyle==1] <- "2"
d1$f[d1$report==1] <- "1"
d1$f[d1$government==1] <- "1"
d1$f[d1$`dont know`== 1] <- "0"
d1$f = as.numeric(d1$f)
View(d1)
d1$f = 0
d1$f[d1$event==1] <- "1"
d1$f[d1$entertainment==1] <- "2"
d1$f[d1$lifestyle==1] <- "3"
d1$f[d1$report==1] <- "4"
d1$f[d1$government==1] <- "5"
d1$f[d1$`dont know`== 1] <- "0"
d1$f = as.numeric(d1$f)
View(d1)
####2 RUN the classifier models
library(quanteda)
library(caret)
library(e1071)
View(d1)
#isolate the vars to test
#sub-sample text w/coding scheme
d2 = d1[c(11,12)]
#create a corpus for the  machine counts and assign a document variable
c1 = corpus(d2$text2)
summary(c1)
#create the document feature matrix
dtm = dfm(c1)
#create training (n = 600) and test (N = 150) data sets (*larger training sets = better models)
train_dtm = dfm_sample(dtm, size = 600)
test_dtm = dtm[setdiff(docnames(dtm), docnames(train_dtm)), ]
#specify our models
nb_model <- textmodel_nb(train_dtm, docvars(train_dtm, "f"))
####2 RUN the classifier models
library(quanteda)#text analysis
library(quanteda.textstats)#newer package for stats
library(caret)#predictive models
library(e1071)#probability testing, TU Wien
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
